{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDQN aplicado al entorno de Lunar Lander v2 (OpenAI-gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir la clase Agente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Actualmente se presenta una red de tres capas (input, hidden, y output); sin embargo, durante los experimentos se probaron diferentes topologias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "    def __init__(self, state_size, action_size,gamma,lr,ep_dec):\n",
    "        self.state_size = state_size      \n",
    "        self.action_size = action_size    \n",
    "        self.memory = deque(maxlen=3000) \n",
    "        self.gamma = gamma                 \n",
    "        self.learning_rate = lr       \n",
    "        \n",
    "        self.epsilon = 1.0              \n",
    "        self.epsilon_min = 0.01         \n",
    "        self.epsilon_decay = ep_dec       \n",
    "        self.model = self._build_model() \n",
    "        \n",
    "        self.target_model = self._build_model() \n",
    "        self.update_target_model() \n",
    "       \n",
    "\n",
    "    def _build_model(self):\n",
    "       \n",
    "        model = Sequential() \n",
    "        model.add(Dense(20, input_dim=self.state_size, activation='relu')) \n",
    "        model.add(Dense(20, activation='relu')) \n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "       \n",
    "        model.compile(loss='mse', optimizer = Adam(lr=self.learning_rate)) \n",
    "        return model\n",
    "    \n",
    "    # Metodo para copiar los pesos de model a target_model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done): \n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    " \n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return random.randrange(self.action_size)\n",
    "        action_values = self.model.predict(state)\n",
    "        return np.argmax(action_values[0]) \n",
    "\n",
    "    def replay(self, batch_size): \n",
    "        minibatch = random.sample(self.memory, batch_size) \n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            \n",
    "            target = self.model.predict(state)\n",
    "            \n",
    "            if done:\n",
    "                target[0][action] = reward   \n",
    "            else: \n",
    "                Qvals_next_state = self.target_model.predict(next_state)[0]\n",
    "                target[0][action] = reward + self.gamma * np.amax(Qvals_next_state)\n",
    " \n",
    "            self.model.fit(state, target, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min: \n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir los hiperparametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "batch_size = 200 \n",
    "EPISODES = 100\n",
    "GAMMA=0.95\n",
    "LEARNING_RATE=0.01\n",
    "EP_DEC=0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2') #Nota, para poder trabajar con el enviroment LunarLander hay que instalar la extencion Box2D de gym\n",
    "#Utilizar pip install Box2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DDQNAgent(state_size, action_size,GAMMA,LEARNING_RATE,EP_DEC)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0/100, score: -191.6576480280395, e: 1.0\n",
      "episode: 1/100, score: -139.0151978894143, e: 1.0\n",
      "episode: 2/100, score: -59.85937805177992, e: 1.0\n",
      "episode: 3/100, score: -77.55286099637524, e: 0.99\n",
      "episode: 4/100, score: -306.97077207199413, e: 0.99\n",
      "episode: 5/100, score: -85.1202872139838, e: 0.99\n",
      "episode: 6/100, score: -306.91651888407443, e: 0.98\n",
      "episode: 7/100, score: -310.7851156774142, e: 0.98\n",
      "episode: 8/100, score: -113.09521252164593, e: 0.97\n",
      "episode: 9/100, score: -122.61248263248723, e: 0.97\n",
      "episode: 10/100, score: -258.0329200762476, e: 0.96\n",
      "episode: 11/100, score: -101.74740239372957, e: 0.96\n",
      "episode: 12/100, score: -320.5832839249805, e: 0.95\n",
      "episode: 13/100, score: -91.95386178686606, e: 0.95\n",
      "episode: 14/100, score: -384.82634823168024, e: 0.94\n",
      "episode: 15/100, score: -218.26939388201356, e: 0.94\n",
      "episode: 16/100, score: -204.1476064158213, e: 0.93\n",
      "episode: 17/100, score: -396.24144612985725, e: 0.93\n",
      "episode: 18/100, score: -114.16517757226056, e: 0.92\n",
      "episode: 19/100, score: -477.68580027694975, e: 0.92\n",
      "episode: 20/100, score: -165.38595326416345, e: 0.91\n",
      "episode: 21/100, score: -127.37713779908192, e: 0.91\n",
      "episode: 22/100, score: -120.36073251292837, e: 0.9\n",
      "episode: 23/100, score: -300.4340301350443, e: 0.9\n",
      "episode: 24/100, score: -229.3556630963535, e: 0.9\n",
      "episode: 25/100, score: -220.25054044788033, e: 0.89\n",
      "episode: 26/100, score: -43.31418843967488, e: 0.89\n",
      "episode: 27/100, score: -60.426464604729375, e: 0.88\n",
      "episode: 28/100, score: -181.4062314984398, e: 0.88\n",
      "episode: 29/100, score: -93.46426556906206, e: 0.87\n",
      "episode: 30/100, score: -307.9342717265801, e: 0.87\n",
      "episode: 31/100, score: -126.96288605221628, e: 0.86\n",
      "episode: 32/100, score: -564.7110725320414, e: 0.86\n",
      "episode: 33/100, score: -289.5503993578758, e: 0.86\n",
      "episode: 34/100, score: -241.67010614309083, e: 0.85\n",
      "episode: 35/100, score: -188.4395728549834, e: 0.85\n",
      "episode: 36/100, score: -62.89167606805876, e: 0.84\n",
      "episode: 38/100, score: -93.7157082386627, e: 0.83\n",
      "episode: 39/100, score: -103.27933193685048, e: 0.83\n",
      "episode: 40/100, score: -31.998990085882298, e: 0.83\n",
      "episode: 41/100, score: -104.24446206265539, e: 0.82\n",
      "episode: 42/100, score: -176.3436215662411, e: 0.82\n",
      "episode: 43/100, score: -61.32117379902153, e: 0.81\n",
      "episode: 44/100, score: -154.5453720257095, e: 0.81\n",
      "episode: 45/100, score: -108.1367858092255, e: 0.81\n",
      "episode: 46/100, score: -384.99437503440436, e: 0.8\n",
      "episode: 47/100, score: -128.30403096532234, e: 0.8\n",
      "episode: 48/100, score: -319.3769732508367, e: 0.79\n",
      "episode: 49/100, score: -245.13910780843807, e: 0.79\n",
      "episode: 50/100, score: -489.17290258822754, e: 0.79\n",
      "episode: 51/100, score: -75.84226677807018, e: 0.78\n",
      "episode: 52/100, score: -35.35469465099524, e: 0.78\n",
      "episode: 53/100, score: -21.497198012074904, e: 0.77\n",
      "episode: 54/100, score: -72.34636971176924, e: 0.77\n",
      "episode: 55/100, score: -129.75184862660274, e: 0.77\n",
      "episode: 56/100, score: -118.38311576909672, e: 0.76\n",
      "episode: 57/100, score: -35.74251806277526, e: 0.76\n",
      "episode: 58/100, score: -108.23654145740366, e: 0.76\n",
      "episode: 59/100, score: -138.4508836403893, e: 0.75\n",
      "episode: 60/100, score: -462.9206595365185, e: 0.75\n",
      "episode: 61/100, score: -222.69776545132527, e: 0.74\n",
      "episode: 62/100, score: -96.6722886009868, e: 0.74\n",
      "episode: 63/100, score: -115.68763816023406, e: 0.74\n",
      "episode: 64/100, score: -92.6510440844898, e: 0.73\n",
      "episode: 65/100, score: -89.76519032718514, e: 0.73\n",
      "episode: 66/100, score: -113.91063028470143, e: 0.73\n",
      "episode: 67/100, score: -156.03964115735727, e: 0.72\n",
      "episode: 68/100, score: -67.57651226923011, e: 0.72\n",
      "episode: 69/100, score: -86.53057285860474, e: 0.71\n",
      "episode: 70/100, score: -58.28847706049547, e: 0.71\n",
      "episode: 71/100, score: -89.63979193956875, e: 0.71\n",
      "episode: 72/100, score: -79.73330756272763, e: 0.7\n",
      "episode: 73/100, score: -334.9008696374385, e: 0.7\n",
      "episode: 75/100, score: -58.954073554634235, e: 0.69\n",
      "episode: 76/100, score: -98.49763563750977, e: 0.69\n",
      "episode: 77/100, score: -102.72778808036584, e: 0.69\n",
      "episode: 78/100, score: -135.3062496260677, e: 0.68\n",
      "episode: 79/100, score: -97.68586403534067, e: 0.68\n",
      "episode: 80/100, score: -68.90284093673534, e: 0.68\n",
      "episode: 81/100, score: -20.696079377696023, e: 0.67\n",
      "episode: 82/100, score: -463.18067753925135, e: 0.67\n",
      "episode: 83/100, score: -115.54328185221685, e: 0.67\n",
      "episode: 84/100, score: -63.66727668626883, e: 0.66\n",
      "episode: 85/100, score: -174.4150549856656, e: 0.66\n",
      "episode: 86/100, score: -45.86631316517036, e: 0.66\n",
      "episode: 87/100, score: -80.71705933866332, e: 0.65\n",
      "episode: 88/100, score: -225.91858956282084, e: 0.65\n",
      "episode: 89/100, score: -64.12741587775184, e: 0.65\n",
      "episode: 90/100, score: -159.76432022876924, e: 0.64\n",
      "episode: 91/100, score: -74.45294314020535, e: 0.64\n",
      "episode: 92/100, score: -34.08466488758823, e: 0.64\n",
      "episode: 93/100, score: -67.08368834137877, e: 0.63\n",
      "episode: 94/100, score: -198.441211210054, e: 0.63\n",
      "episode: 95/100, score: -84.00020572244595, e: 0.63\n",
      "episode: 96/100, score: -96.0374150853622, e: 0.62\n",
      "episode: 97/100, score: -154.6497486634413, e: 0.62\n",
      "episode: 98/100, score: -16.85135452374317, e: 0.62\n",
      "episode: 99/100, score: -154.32481478313747, e: 0.61\n",
      "235.35822749137878\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "for e in range(EPISODES): \n",
    "    state = env.reset() \n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    acc_s=0\n",
    "    #La variable acc_s se utilizara para guardar un acumulado de los rewards conseguidos en cada paso, el score se basara en este acumulado\n",
    "    for step in range(300):  #En este enviroment se utiliza solamente 300 steps para que la nave no se estanque volando sin rumbo\n",
    "        env.render()  \n",
    "        action = agent.get_action(state)  \n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        acc_s+=reward\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done) \n",
    "        state = next_state  \n",
    "        if done: \n",
    "            agent.update_target_model()  \n",
    "            print(\"episode: {}/{}, score: {}, e: {:.2}\".format(e, EPISODES, acc_s, agent.epsilon))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size: \n",
    "        agent.replay(batch_size)\n",
    "env.close()\n",
    "print(time()-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba con el modelo entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_Llander(agent, trials = 1):\n",
    "    env = gym.make('LunarLander-v2')  \n",
    "    scores = []\n",
    "    for trial in range(trials):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        state = []\n",
    "        env.reset()\n",
    "        for step in range(300):\n",
    "            env.render()\n",
    "            if len(state) == 0:   \n",
    "                action = random.randrange(0,4)\n",
    "            else:\n",
    "                action_values = agent.model.predict(state.reshape(1, 8)) \n",
    "                action = np.argmax(action_values[0])     \n",
    "\n",
    "            next_state, reward, done, _  = env.step(action) \n",
    "            score += reward \n",
    "            state = next_state\n",
    "            game_memory.append([next_state, action])\n",
    "            if done: \n",
    "                break\n",
    "        print(\"Play {}/{}, score: {}\".format(trial, trials, score))\n",
    "        scores.append(score)\n",
    "    env.close()\n",
    "    print(\"Score medio = {}\".format(sum(scores) /float(trials)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Play 0/5, score: -158.70174858335616\n",
      "Play 1/5, score: -144.3998070516402\n",
      "Play 2/5, score: -179.1531633763765\n",
      "Play 3/5, score: -150.24641340696425\n",
      "Play 4/5, score: -165.01081464681744\n",
      "Score medio = -159.50238941303093\n"
     ]
    }
   ],
   "source": [
    "TRIALS=5\n",
    "play_Llander(agent,TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'scores.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2e7747056f22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'scores.txt'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'scores = {dct}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'scores.txt'"
     ]
    }
   ],
   "source": [
    "with open('scores.txt') as f:\n",
    "    dct = f.read()\n",
    "    exec(f'scores = {dct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Para suavizar las curvas de aprendizaje, se empleó\n",
    "# una media movil de N términos.\n",
    "N = 100\n",
    "\n",
    "# Mostrando las curvas\n",
    "plt.figure(figsize=(8, 5))\n",
    "for (lr, gamma), score_hist in scores.items():\n",
    "    smooth = np.convolve(score_hist, np.ones((N,)) / N, mode='valid')\n",
    "    plt.plot(smooth, label=fr'$lr={lr}, \\gamma={gamma}$', lw=3)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Score obtenido')\n",
    "plt.title(r'Grid search para $lr$, $\\gamma$ con $\\epsilon{decay}=0.995$ constante')\n",
    "plt.savefig('gs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
